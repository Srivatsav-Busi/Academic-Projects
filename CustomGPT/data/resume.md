# Srivatsav Busi - ML Engineer & Data Engineering Specialist

## Contact Information
- **Email**: srivatsavbusi@gmail.com
- **Phone**: 4708909738
- **Location**: Atlanta, GA
- **GitHub**: github.com/srivatsavbusi

## Professional Summary

Seven years of hands-on experience in AI/ML, data engineering, and process improvement. Currently working as ML Engineer at Yahoo, with expertise in building end-to-end ML pipelines, data processing, and cloud-based solutions. Looking for a role in an organization that leverages my skills for continued learning and growth.

## Core Competencies

### Technical Skills
- **Programming**: Python, R, SQL, PySpark, Shell Scripting (Linux)
- **Frameworks**: Apache Spark, Pandas, NumPy, Flask, Scikit-learn, Matplotlib, Seaborn, NLTK
- **Tools**: Airflow, Apache NiFi, Tableau, Git, Terraform, NoSQL (Mongo, Cassandra), RDBMS, Microservices
- **Big Data & Compute**: BigQuery, Vertex AI, Dataflow, MapReduce, EMR, GCP, AWS, Azure
- **Technical Skills**: Data Ingestion, ETL, Data Modelling, Redshift, DynamoDB, Resource Optimization, Visualization
- **Methodologies**: Agile, Scrum

## Professional Experience

### ML Engineer | Yahoo | San Francisco, CA | Mar 2024 - Present
- **Designed, developed, and optimized end-to-end ML pipelines** using KFP (Kubeflow Pipelines), incorporating lightweight components for training, evaluation, and model promotion workflows
- **Developed modular and reusable lightweight components** for tasks like data ingestion, TFRecord processing, and model evaluation, ensuring seamless integration into existing pipelines
- **Implemented automated artifact promotion workflows**, gating releases based on evaluation pipeline results, ensuring consistent quality across Dev, PPD, and PRD environments
- **Developed and optimized automated data pipelines** to load diverse file formats (TFRecord, Parquet, CSV, JSON) from Google Cloud Storage into BigQuery, implementing advanced configurations such as schema autodetection, custom partitioning, and data format conversion
- **Leveraged Google Cloud Platform (GCP) services**, including BigQuery, Cloud Storage, and Vertex AI, to build scalable, cost-effective machine learning solutions
- **Collaborated with cross-functional teams**, including data scientists, DevOps engineers, and product managers, while mentoring junior team members on KFP pipeline development

### Data Engineer Intern | Insight Global | Atlanta, GA | Aug 2023 – Dec 2023
- **Developed and implemented high-performance relational databases**, ensuring data integrity and optimal performance through adherence to normalization principles
- **Leveraged Neo4j's graph database capabilities** to develop efficient route selection algorithms, optimizing delivery times and costs for complex logistics networks
- **Harnessed Azure Data Factory** to orchestrate seamless data transfers from on-premises to Azure Data Lake Storage Gen2

### Senior Data Engineer | Experian | Hyderabad, India | Jan 2018 - Dec 2022
- **Built end-to-end decryption workflows** using AWS Step Functions, Lambda, S3, and EMR, automating secure file ingestion and processing for datasets
- **Engineered Lambda-based preprocessors** to parse manifests and remove PGP trailers, ensuring downstream readiness and accurate record validation in AWS-based ETL flows
- **Used Amazon EMR** to process encrypted batches and emit marker files upon successful decryption, enabling event-driven orchestration with downstream services
- **Created validation checks** in AWS Step Functions and Lambda to match decrypted record counts against manifests, minimizing SLA breaches and pipeline failures
- **Parsed Docker image scan outputs** using Python on Lambda, autogenerated secure RUN apt upgrade scripts, and integrated patch logic into CI/CD pipelines
- **Tuned Lambda memory/time configurations** and parallelized steps in AWS Step Functions, reducing latency for decryption workloads across multiple cloud vendors
- **Orchestrated decrypted outputs** into broader AWS Glue and Athena pipelines, enabling real-time querying and compliance-ready data access

### Data Engineer | Waste Management | Hyderabad, India | Nov 2016 – Dec 2017
- **Spearheaded full project cycles** from ideation to maintenance using Agile and Waterfall methodologies
- **Leveraged Python libraries** like NumPy, pandas, and Matplotlib for data engineering and visualization tasks
- **Utilized cloud-based services**, particularly AWS, including EKS and Lambda, for deploying and managing machine learning solutions

## Education

### Master of Science in Information Systems: Big Data Analytics and Management
**Georgia State University - J.Mack Robinson College of Business** | Atlanta, GA | Jan 2023 - Dec 2023

### Bachelor of Technology in Engineering
**National Institute of Technology** | Nagpur, India | Jul 2012 - May 2016

## Certifications
- **Microsoft Azure Data Engineering Associate (DP-203) Certificate**
- **AWS Solution Architect Professional**

## Key Achievements
- **ML Pipeline Optimization**: Built scalable ML pipelines at Yahoo using Kubeflow Pipelines
- **Data Security**: Developed secure decryption workflows at Experian handling sensitive data
- **Cloud Migration**: Successfully migrated data processing workflows to AWS and GCP
- **Cost Optimization**: Reduced infrastructure costs through efficient resource utilization
- **Team Mentoring**: Guided junior team members in ML pipeline development and best practices
